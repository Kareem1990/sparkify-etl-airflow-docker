version: '3'

services:
  # 🛢️ PostgreSQL database used by Airflow for metadata storage
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - airflow

  # ⚙️ Initializes the Airflow database and creates the admin user
  airflow-init:
    image: apache/airflow:2.7.3
    depends_on:
      - postgres
    env_file: .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__SECRET_KEY: 'this_is_a_demo_secret'
    entrypoint: >
      /bin/bash -c "
      airflow db init &&
      airflow users create --username KareemRizk --firstname Kareem --lastname Rizk --role Admin --email kareem.magdy5@example.com --password password
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./sql:/opt/airflow/sql
      - ./.aws_credentials.json:/opt/airflow/secrets/.aws_credentials.json
    networks:
      - airflow

  # 🌐 Airflow Webserver accessible at http://localhost:8080
  webserver:
    image: apache/airflow:2.7.3
    restart: always
    depends_on:
      - postgres
    env_file: .env
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__SECRET_KEY: 'this_is_a_demo_secret'
      AIRFLOW_CONN_REDSHIFT: >-
        postgresql://${REDSHIFT_USER}:${REDSHIFT_PASSWORD}@${REDSHIFT_HOST}:5439/${REDSHIFT_DB}?sslmode=require
      AIRFLOW_CONN_AWS_CREDENTIALS: >-
        aws://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}@
    command: webserver
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./sql:/opt/airflow/sql
      - ./.aws_credentials.json:/opt/airflow/secrets/.aws_credentials.json
    networks:
      - airflow

  # ⏱️ Airflow scheduler that triggers tasks according to DAG schedules
  scheduler:
    image: apache/airflow:2.7.3
    restart: always
    depends_on:
      - webserver
    env_file: .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: 'this_is_a_demo_secret'
      AIRFLOW_CONN_REDSHIFT: >-
        postgresql://${REDSHIFT_USER}:${REDSHIFT_PASSWORD}@${REDSHIFT_HOST}:5439/${REDSHIFT_DB}?sslmode=require
      AIRFLOW_CONN_AWS_CREDENTIALS: >-
        aws://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}@
    command: scheduler
    volumes:
      - ./dags:/opt/airflow/dags
      - ./plugins:/opt/airflow/plugins
      - ./sql:/opt/airflow/sql
      - ./.aws_credentials.json:/opt/airflow/secrets/.aws_credentials.json
    networks:
      - airflow

volumes:
  postgres-db-volume:

networks:
  airflow:
# docker-compose.yaml
#
# This file defines a complete local Airflow development environment using Docker Compose.
# It provisions the following services:
#   - PostgreSQL: Metadata database for Airflow
#   - Airflow Init: Initializes the Airflow database and creates an admin user
#   - Airflow Webserver: Provides the Airflow UI at http://localhost:8080
#   - Airflow Scheduler: Executes scheduled DAG tasks
#
# The setup also mounts your local DAGs, plugins, and SQL scripts into the containers,
# and injects AWS/Redshift credentials via environment variables for seamless integration
# with AWS Redshift Serverless and S3.